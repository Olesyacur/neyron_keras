{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras import utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "imdb_dir = '/content/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts  =[]\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "# Токенизация текста из исходного набора данных\n",
    "maxlen = 100 # Отсечение остатка отзывовпосле 100 слова\n",
    "training_samples = 200 # Обучение на выборке из 200 образцов\n",
    "validation_samples = 10000 # Проверка на выборке из 10 000 образцов\n",
    "max_words = 10000 # Рассм только 10 тыс наиболее часто использ-х слов\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = utils.data_utils.pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor',data.shape)\n",
    "print('Shape of label tensor', labels.shape)\n",
    "\n",
    "#Разбивка данных на обучающую и проверочные выборки, но перед этим данные перемешиваются\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples+validation_samples]\n",
    "y_val = labels[training_samples: training_samples+validation_samples]\n",
    "\n",
    "\n",
    "# Теперь векторное представление Glove word-embedding file \n",
    "glove_dir = ''\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove-6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors' %len(embeddings_index))\n",
    "\n",
    "# Подготовка матрицы векторных представлений слов glove\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None :\n",
    "            # Словам, кот нет в индексе представлений, будут соотв-ть нулвые векторы\n",
    "            embedding_matrix[i]  = embedding_vector\n",
    "    \n",
    "# Определение модели\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "# Загрузка предварительно обученных векторных представлений слов в слой еmbeddings\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False # заморозка слоя\n",
    "\n",
    "# Обучение и оценка модели\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "model.save_weights('pre_trained_glove_model.h5')\n",
    "\n",
    "# Вывод результатов\n",
    "import matplotlib.pyplot as plt \n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Обучение той же модели без обученных векторных представлений \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc']\n",
    ")\n",
    "history= model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "# Токенизация данных из контрольной выборки \n",
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] =='.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type=='neg':\n",
    "                labels.append[0]\n",
    "            else:\n",
    "                labels.append[1]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences[texts]\n",
    "x_test = utils.data_utils.pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)\n",
    "\n",
    "# Оценка модели на контрольном наборе данных\n",
    "model.load_weights('pre-trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
