{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "from keras.datasets import imdb \n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "\n",
    "'''Рекуррентные нейронные сети.'''\n",
    "\n",
    "# Реализация сети RNN на основе Numpy\n",
    "timesteps = 100 # Число временных интервалов во вх посл-ти\n",
    "input_features = 32 # Размерность пространства вх признаков\n",
    "output_features = 64 # Размерность пространства вых признаков\n",
    "\n",
    "# Входные данные: случайный шум для простоты примера\n",
    "inputs = np.random.random((timesteps, input_features))\n",
    "\n",
    "# Начальное состояние: вектор с нулевыми значениями элементов\n",
    "state_t = np.zeros((output_features, ))\n",
    "\n",
    "# Создание матриц со случайными весами\n",
    "W = np.random.random((output_features, input_features))\n",
    "U = np.random.random((output_features, output_features))\n",
    "b = np.random.random((output_features))\n",
    "\n",
    "successive_outputs = []\n",
    "for input_t in inputs:\n",
    "    # Объед-ние вх данных с текущем состоянием (вых данными на пред. шаге)\n",
    "    output_t = np.tanh(np.dot(W, input_t)+np.dot(U, state_t)+b)\n",
    "    \n",
    "    # Сохранение вых данных в список\n",
    "    successive_outputs.append(output_t)\n",
    "\n",
    "    # Обновление текущего состояния\n",
    "    state_t = output_t\n",
    "\n",
    "# Окончательный рез-т - 2мерный тензор (времен. интервалы, вх признаки)\n",
    "final_output_sequence = np.concatenate(successive_outputs, axis=0)\n",
    "\n",
    "'''Рекуррентный слой в Керас.'''\n",
    "\n",
    "model =Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()\n",
    "\n",
    "# Подготовка данных IMDB\n",
    "\n",
    "max_features = 10000 # Кол-во слов рассм-х как признаки\n",
    "maxlen = 500 # Обрезка текста после этого кол-ва слов\n",
    "batch_size = 32 \n",
    "\n",
    "print(\"Loading data\")\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(\n",
    "    num_words=max_features)\n",
    "print(len(input_train), \"train sequences\")\n",
    "print(len(input_test), \"test sequences\")\n",
    "\n",
    "print(\"pad sequences (samples x time)\")\n",
    "input_train = utils.data_utils.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = utils.data_utils.pad_sequences(input_test, maxlen=maxlen)\n",
    "print(\"input_train shape:\", input_train.shape)\n",
    "print(\"input_test shape:\", input_test.shape)\n",
    "\n",
    "# Обучение модели со слоями Embedding и SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "    input_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Вывод результатов\n",
    "import matplotlib.pyplot as plt \n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy ')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Реализация архитектуры LSTM в псевдокоде (1/2)\n",
    "# псевдокод LSTM\n",
    "# output_t = activation(dot(state_t, Uo)+dot(input_t, Wo)+dot(C_t, Vo)+ bo)\n",
    "# i_t = activation(dot(state_t, Ui)+dot(input_t, Wl)+bi)\n",
    "# f_t = activation(dot(state_t, Uf)+dot(input_t, Wf)+bf)\n",
    "# k_t = activation(dot(state_t, Uk)+dot(input_t, Wk)+bk)\n",
    "# c_t+1 = i_t+k_t+c_t*f_t\n",
    "\n",
    "# использование слоя LSTM из Керас\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "    input_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Plotting results\n",
    "import matplotlib.pyplot as plt \n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy ')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
